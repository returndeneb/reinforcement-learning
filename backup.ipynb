{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def get_canonical_form(board):\n",
    "    \"\"\"보드의 8가지 대칭(회전, 대칭) 중 정규형(canonical form)을 찾습니다.\"\"\"\n",
    "    symmetries = []\n",
    "    current_board = board.copy()\n",
    "    for _ in range(4): # 4 rotations\n",
    "        symmetries.append(current_board)\n",
    "        symmetries.append(np.fliplr(current_board))\n",
    "        current_board = np.rot90(current_board)\n",
    "\n",
    "    # 튜플로 변환하여 정렬 가능하게 만듦\n",
    "    symmetries_as_tuples = [tuple(b.flatten()) for b in symmetries]\n",
    "    canonical_tuple = min(symmetries_as_tuples)\n",
    "    return np.array(canonical_tuple).reshape(3, 3)\n",
    "\n",
    "memoized_policy = {}\n",
    "def build_optimal_policy_map(board, player):\n",
    "    \"\"\"모든 고유 상태에 대한 최적의 수를 미리 계산하여 맵에 저장 (재귀)\"\"\"\n",
    "    canonical_board = get_canonical_form(board)\n",
    "    canonical_tuple = tuple(canonical_board.flatten())\n",
    "\n",
    "    if canonical_tuple in memoized_policy or TicTacToe()._check_winner_on_board(board) is not None:\n",
    "        return\n",
    "\n",
    "    game = TicTacToe()\n",
    "    game.board = board\n",
    "    available_actions = game.get_available_actions()\n",
    "    \n",
    "    best_score = -math.inf\n",
    "    best_moves = []\n",
    "    \n",
    "    for action in available_actions:\n",
    "        new_board = board.copy()\n",
    "        row, col = action // 3, action % 3\n",
    "        new_board[row, col] = player\n",
    "        score = -minimax(new_board, -player)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_moves = [action]\n",
    "        elif score == best_score:\n",
    "            best_moves.append(action)\n",
    "\n",
    "    memoized_policy[tuple(board.flatten())] = best_moves\n",
    "    \n",
    "    for move in best_moves:\n",
    "        next_board = board.copy()\n",
    "        row, col = move // 3, move % 3\n",
    "        next_board[row, col] = player\n",
    "        build_optimal_policy_map(next_board, -player)\n",
    "\n",
    "# TicTacToe 클래스 내부에 헬퍼 함수 추가\n",
    "TicTacToe._check_winner_on_board = lambda self, board: TicTacToe.check_winner(type('obj', (object,), {'board': board})())\n",
    "\n",
    "\n",
    "def generate_all_optimal_trajectories():\n",
    "    \"\"\"사전 계산된 정책을 사용하여 모든 최적의 경로를 생성합니다.\"\"\"\n",
    "    print(\"Pre-calculating optimal policy for all unique states...\")\n",
    "    start_board = np.zeros((3, 3), dtype=int)\n",
    "    build_optimal_policy_map(start_board, 1)\n",
    "    \n",
    "    print(\"Generating all unique optimal trajectories...\")\n",
    "    all_trajectories = []\n",
    "    \n",
    "    def find_paths(board, player, path):\n",
    "        winner = TicTacToe()._check_winner_on_board(board)\n",
    "        if winner is not None:\n",
    "            states, actions = zip(*path) if path else ([], [])\n",
    "            rewards = np.zeros(len(actions))\n",
    "            \n",
    "            # 승패에 따라 Reward-to-go 계산\n",
    "            rewards_to_go = np.zeros_like(rewards, dtype=float)\n",
    "            final_reward = winner # 1 for win, -1 for loss, 0 for draw\n",
    "            for t in range(len(rewards)):\n",
    "                rewards_to_go[t] = final_reward\n",
    "                \n",
    "            all_trajectories.append({\n",
    "                'states': np.array(states),\n",
    "                'actions': np.array(actions),\n",
    "                'rewards_to_go': rewards_to_go\n",
    "            })\n",
    "            return\n",
    "\n",
    "        optimal_moves = memoized_policy.get(tuple(board.flatten()))\n",
    "        if not optimal_moves: return\n",
    "\n",
    "        for move in optimal_moves:\n",
    "            next_board = board.copy()\n",
    "            row, col = move // 3, move % 3\n",
    "            next_board[row, col] = player\n",
    "            find_paths(next_board, -player, path + [(board.flatten(), move)])\n",
    "\n",
    "    find_paths(start_board, 1, [])\n",
    "    return all_trajectories\n",
    "\n",
    "\n",
    "# --- 3. PyTorch Dataset & Decision Transformer 모델 (이전과 동일) ---\n",
    "class TicTacToeDataset(Dataset):\n",
    "    def __init__(self, trajectories, context_length):\n",
    "        self.trajectories = trajectories\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = len(traj['states'])\n",
    "        start_idx = random.randint(0, traj_len - 1)\n",
    "        states = traj['states'][start_idx : start_idx + self.context_length]\n",
    "        actions = traj['actions'][start_idx : start_idx + self.context_length]\n",
    "        rtgs = traj['rewards_to_go'][start_idx : start_idx + self.context_length]\n",
    "        T = len(states)\n",
    "        padding_len = self.context_length - T\n",
    "        states = torch.tensor(np.pad(states, ((0, padding_len), (0, 0)), 'constant'), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.pad(actions, (0, padding_len), 'constant', constant_values=-1), dtype=torch.long)\n",
    "        rtgs = torch.tensor(np.pad(rtgs, (0, padding_len), 'constant'), dtype=torch.float32).unsqueeze(1)\n",
    "        mask = torch.cat([torch.ones(T), torch.zeros(padding_len)], dim=0)\n",
    "        return states, actions, rtgs, mask\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_head, n_layer, d_model, context_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.embed_state = nn.Linear(state_dim, d_model)\n",
    "        self.embed_action = nn.Embedding(action_dim + 1, d_model)\n",
    "        self.embed_rtg = nn.Linear(1, d_model)\n",
    "        self.embed_timestep = nn.Embedding(context_length, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "        self.predict_action = nn.Linear(d_model, action_dim)\n",
    "\n",
    "    def forward(self, states, actions, rtgs, timesteps):\n",
    "        action_embeddings = self.embed_action(actions + 1)\n",
    "        state_embeddings = self.embed_state(states)\n",
    "        rtg_embeddings = self.embed_rtg(rtgs)\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "        state_embeddings += time_embeddings\n",
    "        action_embeddings += time_embeddings\n",
    "        rtg_embeddings += time_embeddings\n",
    "        stacked_inputs = torch.stack(\n",
    "            (rtg_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(states.shape[0], 3 * self.context_length, self.d_model)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(3 * self.context_length).to(states.device)\n",
    "        encoder_output = self.transformer_encoder(stacked_inputs, mask=causal_mask)\n",
    "        x = encoder_output[:, 1::3, :]\n",
    "        action_preds = self.predict_action(x)\n",
    "        return action_preds\n",
    "\n",
    "# --- 4. 학습 및 평가 (하이퍼파라미터 조정) ---\n",
    "def train():\n",
    "    # Hyperparameters (조정됨)\n",
    "    CONTEXT_LENGTH = 9  # 최대 9수면 게임이 끝나므로\n",
    "    N_EPOCHS = 30       # 데이터가 고품질이므로 에포크 감소\n",
    "    BATCH_SIZE = 64     # 데이터셋 크기에 맞춰 배치 사이즈 조정\n",
    "    LR = 1e-4\n",
    "    D_MODEL = 128\n",
    "    N_HEAD = 4\n",
    "    N_LAYER = 3\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trajectories = generate_all_optimal_trajectories()\n",
    "    end_time = time.time()\n",
    "    print(f\"Generated {len(trajectories)} unique optimal trajectories in {end_time - start_time:.2f} seconds.\")\n",
    "    \n",
    "    dataset = TicTacToeDataset(trajectories, context_length=CONTEXT_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = DecisionTransformer(\n",
    "        state_dim=9, action_dim=9, n_head=N_HEAD, n_layer=N_LAYER,\n",
    "        d_model=D_MODEL, context_length=CONTEXT_LENGTH\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for states, actions, rtgs, mask in dataloader:\n",
    "            states, actions, rtgs, mask = states.to(device), actions.to(device), rtgs.to(device), mask.to(device)\n",
    "            timesteps = torch.arange(CONTEXT_LENGTH, device=device).repeat(states.shape[0], 1)\n",
    "            action_preds = model(states, actions, rtgs, timesteps)\n",
    "            action_preds = action_preds.reshape(-1, 9)\n",
    "            actions_target = actions.reshape(-1)\n",
    "            mask = mask.reshape(-1).bool()\n",
    "            action_preds = action_preds[mask]\n",
    "            actions_target = actions_target[mask]\n",
    "            valid_targets = actions_target != -1\n",
    "            loss = loss_fn(action_preds[valid_targets], actions_target[valid_targets])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "def play_game_with_model(model, context_length):\n",
    "    model.eval()\n",
    "    \n",
    "    # 모델이 현재 사용 중인 device를 가져옵니다.\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    env = TicTacToe()\n",
    "    \n",
    "    # 모든 텐서를 생성할 때 .to(device)를 붙여줍니다.\n",
    "    states = torch.zeros((1, context_length, 9), dtype=torch.float32, device=device)\n",
    "    actions = torch.full((1, context_length), -1, dtype=torch.long, device=device)\n",
    "    rtgs = torch.zeros((1, context_length, 1), dtype=torch.float32, device=device)\n",
    "    rtgs[0, :, 0] = 1.0 # 목표는 승리(RTG=1)\n",
    "    timesteps = torch.arange(0, context_length, device=device).unsqueeze(0)\n",
    "\n",
    "    print(\"\\n--- New Game: You are 'O' ---\")\n",
    "    turn = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and turn < 9:\n",
    "        board_str = \"\"\n",
    "        for i, cell in enumerate(env.get_state()):\n",
    "            mark = 'X' if cell == 1 else 'O' if cell == -1 else str(i)\n",
    "            board_str += f\" {mark} \"\n",
    "            if (i+1) % 3 == 0:\n",
    "                board_str += \"\\n\" if i < 8 else \"\"\n",
    "                if i < 8: board_str += \"---+---+---\\n\"\n",
    "        print(board_str)\n",
    "\n",
    "        if env.current_player == 1:\n",
    "            print(\"Model's turn ('X')...\")\n",
    "            with torch.no_grad():\n",
    "                # 이제 모델과 입력 텐서가 모두 같은 device에 있습니다.\n",
    "                pred_actions = model(states, actions, rtgs, timesteps)\n",
    "            logits = pred_actions[0, turn, :]\n",
    "            available_actions = env.get_available_actions()\n",
    "            mask = torch.full_like(logits, -float('inf'))\n",
    "            mask[available_actions] = 0\n",
    "            move = (logits + mask).argmax().item()\n",
    "            print(f\"Model chooses action: {move}\")\n",
    "        else:\n",
    "            try:\n",
    "                move = int(input(\"Your turn ('O'). Enter move (0-8): \"))\n",
    "                if move not in env.get_available_actions():\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "                    continue\n",
    "            except (ValueError, IndexError):\n",
    "                print(\"Invalid input. Enter a number between 0 and 8.\")\n",
    "                continue\n",
    "\n",
    "        # 상태 업데이트 시에도 .to(device)가 필요합니다.\n",
    "        if turn < context_length:\n",
    "            current_state_tensor = torch.tensor(env.get_state(), dtype=torch.float32).to(device)\n",
    "            states[0, turn] = current_state_tensor\n",
    "            actions[0, turn] = move\n",
    "        \n",
    "        _, reward, done = env.make_move(move)\n",
    "        \n",
    "        # RTG 업데이트\n",
    "        if turn + 1 < context_length:\n",
    "            rtgs[0, turn+1:] = rtgs[0, turn:-1].clone() # clone()을 사용하여 인플레이스 수정 방지\n",
    "            rtgs[0, turn, 0] -= reward\n",
    "        \n",
    "        turn += 1\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    winner = env.check_winner()\n",
    "    print(\"--- Game Over ---\")\n",
    "    if winner == 1: print(\"Model (X) wins!\")\n",
    "    elif winner == -1: print(\"You (O) win!\")\n",
    "    else: print(\"It's a draw!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
