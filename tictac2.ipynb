{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fded5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Generating expert trajectories using Minimax...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 284/5000 [30:05<8:19:36,  6.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 374\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIt\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms a draw!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     trained_model = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# Save the model if you want\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# torch.save(trained_model.state_dict(), \"tictactoe_dt_model.pth\")\u001b[39;00m\n\u001b[32m    378\u001b[39m \n\u001b[32m    379\u001b[39m     \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# model = DecisionTransformer(...)\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;66;03m# model.load_state_dict(torch.load(\"tictactoe_dt_model.pth\"))\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 241\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    238\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m trajectories = \u001b[43mgenerate_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_trajectories\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m dataset = TicTacToeDataset(trajectories, context_length=CONTEXT_LENGTH)\n\u001b[32m    243\u001b[39m dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mgenerate_trajectories\u001b[39m\u001b[34m(num_trajectories)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Minimax 'X' (player 1) vs slightly random 'O' (player -1)\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_player == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     move = \u001b[43mget_best_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_player\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# To add variety, player 'O' sometimes makes a random move\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m random.random() < \u001b[32m0.3\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mget_best_move\u001b[39m\u001b[34m(board, player)\u001b[39m\n\u001b[32m     90\u001b[39m row, col = action // \u001b[32m3\u001b[39m, action % \u001b[32m3\u001b[39m\n\u001b[32m     91\u001b[39m new_board[row, col] = player\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m score = -\u001b[43mminimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score > best_score:\n\u001b[32m     94\u001b[39m     best_score = score\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mminimax\u001b[39m\u001b[34m(board, player)\u001b[39m\n\u001b[32m     71\u001b[39m row, col = action // \u001b[32m3\u001b[39m, action % \u001b[32m3\u001b[39m\n\u001b[32m     72\u001b[39m new_board[row, col] = player\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m score = -\u001b[43mminimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score > best_score:\n\u001b[32m     75\u001b[39m     best_score = score\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mminimax\u001b[39m\u001b[34m(board, player)\u001b[39m\n\u001b[32m     71\u001b[39m row, col = action // \u001b[32m3\u001b[39m, action % \u001b[32m3\u001b[39m\n\u001b[32m     72\u001b[39m new_board[row, col] = player\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m score = -\u001b[43mminimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score > best_score:\n\u001b[32m     75\u001b[39m     best_score = score\n",
      "    \u001b[31m[... skipping similar frames: minimax at line 73 (3 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mminimax\u001b[39m\u001b[34m(board, player)\u001b[39m\n\u001b[32m     71\u001b[39m row, col = action // \u001b[32m3\u001b[39m, action % \u001b[32m3\u001b[39m\n\u001b[32m     72\u001b[39m new_board[row, col] = player\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m score = -\u001b[43mminimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m score > best_score:\n\u001b[32m     75\u001b[39m     best_score = score\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mminimax\u001b[39m\u001b[34m(board, player)\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m winner * player\n\u001b[32m     68\u001b[39m best_score = -math.inf\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_available_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     70\u001b[39m     new_board = board.copy()\n\u001b[32m     71\u001b[39m     row, col = action // \u001b[32m3\u001b[39m, action % \u001b[32m3\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mTicTacToe.get_available_actions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_available_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m val == \u001b[32m0\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mTicTacToe.get_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.board.flatten()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# --- 1. TicTacToe 게임 환경 ---\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1  # 1: 'X', -1: 'O'\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        return [i for i, val in enumerate(self.get_state()) if val == 0]\n",
    "\n",
    "    def make_move(self, action):\n",
    "        if self.get_state()[action] != 0:\n",
    "            raise ValueError(\"Invalid move\")\n",
    "        \n",
    "        row, col = action // 3, action % 3\n",
    "        self.board[row, col] = self.current_player\n",
    "        \n",
    "        winner = self.check_winner()\n",
    "        done = winner is not None\n",
    "        reward = 0\n",
    "        if done:\n",
    "            if winner == 1: reward = 1\n",
    "            elif winner == -1: reward = -1\n",
    "            else: reward = 0 # Draw\n",
    "\n",
    "        self.current_player *= -1\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Rows and Columns\n",
    "        for i in range(3):\n",
    "            if abs(self.board[i, :].sum()) == 3: return self.board[i, 0]\n",
    "            if abs(self.board[:, i].sum()) == 3: return self.board[0, i]\n",
    "        \n",
    "        # Diagonals\n",
    "        if abs(np.diag(self.board).sum()) == 3: return self.board[0, 0]\n",
    "        if abs(np.diag(np.fliplr(self.board)).sum()) == 3: return self.board[0, 2]\n",
    "        \n",
    "        # Draw\n",
    "        if not np.any(self.board == 0):\n",
    "            return 0\n",
    "            \n",
    "        return None # Game not over\n",
    "\n",
    "# --- 2. 전문가 데이터 생성 (Minimax 알고리즘 사용) ---\n",
    "def minimax(board, player):\n",
    "    game = TicTacToe()\n",
    "    game.board = board\n",
    "    winner = game.check_winner()\n",
    "    if winner is not None:\n",
    "        return winner * player\n",
    "\n",
    "    best_score = -math.inf\n",
    "    for action in game.get_available_actions():\n",
    "        new_board = board.copy()\n",
    "        row, col = action // 3, action % 3\n",
    "        new_board[row, col] = player\n",
    "        score = -minimax(new_board, -player)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "    return best_score if best_score != -math.inf else 0\n",
    "\n",
    "def get_best_move(board, player):\n",
    "    best_score = -math.inf\n",
    "    best_move = -1\n",
    "    game = TicTacToe()\n",
    "    game.board = board\n",
    "    \n",
    "    available_actions = game.get_available_actions()\n",
    "    if not available_actions:\n",
    "        return -1\n",
    "\n",
    "    for action in available_actions:\n",
    "        new_board = board.copy()\n",
    "        row, col = action // 3, action % 3\n",
    "        new_board[row, col] = player\n",
    "        score = -minimax(new_board, -player)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_move = action\n",
    "    return best_move\n",
    "\n",
    "def generate_trajectories(num_trajectories):\n",
    "    print(\"Generating expert trajectories using Minimax...\")\n",
    "    trajectories = []\n",
    "    env = TicTacToe()\n",
    "    \n",
    "    for _ in tqdm(range(num_trajectories)):\n",
    "        states, actions, rewards = [], [], []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            current_player = env.current_player\n",
    "            \n",
    "            # Minimax 'X' (player 1) vs slightly random 'O' (player -1)\n",
    "            if current_player == 1:\n",
    "                move = get_best_move(env.board, current_player)\n",
    "            else:\n",
    "                # To add variety, player 'O' sometimes makes a random move\n",
    "                if random.random() < 0.3:\n",
    "                    move = random.choice(env.get_available_actions())\n",
    "                else:\n",
    "                    move = get_best_move(env.board, current_player)\n",
    "\n",
    "            if move == -1: break # No more moves\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(move)\n",
    "            \n",
    "            state, reward, done = env.make_move(move)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        # Calculate rewards-to-go\n",
    "        rewards_to_go = np.zeros_like(rewards, dtype=float)\n",
    "        running_rtg = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            # The final reward is the outcome of the game.\n",
    "            # We assign it to the player who made the move at that step.\n",
    "            if rewards[t] != 0: running_rtg = rewards[t]\n",
    "            rewards_to_go[t] = running_rtg\n",
    "\n",
    "        trajectories.append({\n",
    "            'states': np.array(states),\n",
    "            'actions': np.array(actions),\n",
    "            'rewards_to_go': rewards_to_go\n",
    "        })\n",
    "    return trajectories\n",
    "\n",
    "# --- 3. PyTorch Dataset & Decision Transformer 모델 ---\n",
    "class TicTacToeDataset(Dataset):\n",
    "    def __init__(self, trajectories, context_length):\n",
    "        self.trajectories = trajectories\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = len(traj['states'])\n",
    "        \n",
    "        # Pick a random starting point in the trajectory\n",
    "        start_idx = random.randint(0, traj_len - 1)\n",
    "        \n",
    "        states = traj['states'][start_idx : start_idx + self.context_length]\n",
    "        actions = traj['actions'][start_idx : start_idx + self.context_length]\n",
    "        rtgs = traj['rewards_to_go'][start_idx : start_idx + self.context_length]\n",
    "\n",
    "        # Padding\n",
    "        T = len(states)\n",
    "        padding_len = self.context_length - T\n",
    "        \n",
    "        states = torch.tensor(np.pad(states, ((0, padding_len), (0, 0)), 'constant'), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.pad(actions, (0, padding_len), 'constant', constant_values=-1), dtype=torch.long)\n",
    "        rtgs = torch.tensor(np.pad(rtgs, (0, padding_len), 'constant'), dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        mask = torch.cat([torch.ones(T), torch.zeros(padding_len)], dim=0)\n",
    "        \n",
    "        return states, actions, rtgs, mask\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_head, n_layer, d_model, context_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        self.embed_state = nn.Linear(state_dim, d_model)\n",
    "        self.embed_action = nn.Embedding(action_dim + 1, d_model) # +1 for padding action\n",
    "        self.embed_rtg = nn.Linear(1, d_model)\n",
    "        self.embed_timestep = nn.Embedding(context_length, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "\n",
    "        self.predict_action = nn.Linear(d_model, action_dim)\n",
    "\n",
    "    def forward(self, states, actions, rtgs, timesteps):\n",
    "        # `actions` need to be handled for padding (-1)\n",
    "        # We shift actions to be non-negative for embedding lookup\n",
    "        action_embeddings = self.embed_action(actions + 1)\n",
    "        state_embeddings = self.embed_state(states)\n",
    "        rtg_embeddings = self.embed_rtg(rtgs)\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # Interleave sequence: (RTG_1, s_1, a_1, RTG_2, s_2, a_2, ...)\n",
    "        # A simpler approach is to sum embeddings with time embeddings\n",
    "        # Here we sum them, which works well for simpler tasks\n",
    "        \n",
    "        state_embeddings += time_embeddings\n",
    "        action_embeddings += time_embeddings\n",
    "        rtg_embeddings += time_embeddings\n",
    "\n",
    "        # This creates the sequence of length 3*K\n",
    "        stacked_inputs = torch.stack(\n",
    "            (rtg_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(states.shape[0], 3 * self.context_length, self.d_model)\n",
    "\n",
    "        # Causal mask to ensure predictions at t only depend on inputs before t\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(3 * self.context_length).to(states.device)\n",
    "        \n",
    "        encoder_output = self.transformer_encoder(stacked_inputs, mask=causal_mask)\n",
    "        \n",
    "        # We only want to predict actions, which are at indices 1, 4, 7, ...\n",
    "        # These correspond to state embeddings\n",
    "        x = encoder_output[:, 1::3, :] # Get embeddings for state positions\n",
    "        \n",
    "        action_preds = self.predict_action(x)\n",
    "        return action_preds\n",
    "\n",
    "# --- 4. 학습 및 평가 ---\n",
    "def train():\n",
    "    # Hyperparameters\n",
    "    CONTEXT_LENGTH = 5  # Max sequence length for the model\n",
    "    N_EPOCHS = 50\n",
    "    BATCH_SIZE = 128\n",
    "    LR = 1e-4\n",
    "    D_MODEL = 128\n",
    "    N_HEAD = 4\n",
    "    N_LAYER = 3\n",
    "\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    trajectories = generate_trajectories(num_trajectories=5000)\n",
    "    dataset = TicTacToeDataset(trajectories, context_length=CONTEXT_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = DecisionTransformer(\n",
    "        state_dim=9, \n",
    "        action_dim=9, \n",
    "        n_head=N_HEAD,\n",
    "        n_layer=N_LAYER,\n",
    "        d_model=D_MODEL,\n",
    "        context_length=CONTEXT_LENGTH\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for states, actions, rtgs, mask in dataloader:\n",
    "            states, actions, rtgs, mask = states.to(device), actions.to(device), rtgs.to(device), mask.to(device)\n",
    "            \n",
    "            timesteps = torch.arange(CONTEXT_LENGTH, device=device).repeat(states.shape[0], 1)\n",
    "            \n",
    "            action_preds = model(states, actions, rtgs, timesteps)\n",
    "            \n",
    "            # We only calculate loss on the actions that were actually taken (not padded)\n",
    "            # Reshape for loss function\n",
    "            action_preds = action_preds.reshape(-1, 9)\n",
    "            actions_target = actions.reshape(-1)\n",
    "            mask = mask.reshape(-1).bool()\n",
    "            \n",
    "            # Filter out padded parts\n",
    "            action_preds = action_preds[mask]\n",
    "            actions_target = actions_target[mask]\n",
    "\n",
    "            # We don't want to compute loss on the padding action value (-1)\n",
    "            valid_targets = actions_target != -1\n",
    "            loss = loss_fn(action_preds[valid_targets], actions_target[valid_targets])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "def play_game_with_model(model, context_length):\n",
    "    model.eval()\n",
    "    env = TicTacToe()\n",
    "    \n",
    "    states = torch.zeros((1, context_length, 9), dtype=torch.float32)\n",
    "    actions = torch.full((1, context_length), -1, dtype=torch.long)\n",
    "    \n",
    "    # Target: Win the game! So, initial RTG is 1.0\n",
    "    rtgs = torch.zeros((1, context_length, 1), dtype=torch.float32)\n",
    "    rtgs[0, -1, 0] = 1.0\n",
    "    \n",
    "    timesteps = torch.arange(0, context_length).unsqueeze(0)\n",
    "\n",
    "    print(\"\\n--- New Game: You are 'O' ---\")\n",
    "    turn = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Print board\n",
    "        board_str = \"\"\n",
    "        for i, cell in enumerate(env.get_state()):\n",
    "            mark = 'X' if cell == 1 else 'O' if cell == -1 else str(i)\n",
    "            board_str += f\" {mark} \"\n",
    "            if (i+1) % 3 == 0:\n",
    "                board_str += \"\\n\" if i < 8 else \"\"\n",
    "                if i < 8: board_str += \"---+---+---\\n\"\n",
    "        print(board_str)\n",
    "\n",
    "        # Model's turn ('X')\n",
    "        if env.current_player == 1:\n",
    "            print(\"Model's turn ('X')...\")\n",
    "            with torch.no_grad():\n",
    "                pred_actions = model(states, actions, rtgs, timesteps)\n",
    "            \n",
    "            # Get the prediction for the current step\n",
    "            logits = pred_actions[0, turn, :]\n",
    "            \n",
    "            # Mask illegal moves\n",
    "            available_actions = env.get_available_actions()\n",
    "            mask = torch.full_like(logits, -float('inf'))\n",
    "            mask[available_actions] = 0\n",
    "            \n",
    "            # Choose best legal move\n",
    "            move = (logits + mask).argmax().item()\n",
    "            \n",
    "            print(f\"Model chooses action: {move}\")\n",
    "\n",
    "        # Human's turn ('O')\n",
    "        else:\n",
    "            try:\n",
    "                move = int(input(\"Your turn ('O'). Enter move (0-8): \"))\n",
    "                if move not in env.get_available_actions():\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Enter a number between 0 and 8.\")\n",
    "                continue\n",
    "\n",
    "        # Update sequence for the model\n",
    "        if turn < context_length:\n",
    "            states[0, turn] = torch.tensor(env.get_state(), dtype=torch.float32)\n",
    "            actions[0, turn] = move\n",
    "        \n",
    "        # Make move in environment\n",
    "        _, reward, done = env.make_move(move)\n",
    "\n",
    "        # Update RTG for next prediction\n",
    "        rtgs[0, turn, 0] = rtgs[0, turn, 0] - reward\n",
    "        \n",
    "        turn += 1\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    winner = env.check_winner()\n",
    "    print(\"--- Game Over ---\")\n",
    "    if winner == 1: print(\"Model (X) wins!\")\n",
    "    elif winner == -1: print(\"You (O) win!\")\n",
    "    else: print(\"It's a draw!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained_model = train()\n",
    "    \n",
    "    # Save the model if you want\n",
    "    # torch.save(trained_model.state_dict(), \"tictactoe_dt_model.pth\")\n",
    "    \n",
    "    # Load the model\n",
    "    # model = DecisionTransformer(...)\n",
    "    # model.load_state_dict(torch.load(\"tictactoe_dt_model.pth\"))\n",
    "    \n",
    "    while True:\n",
    "        play_game_with_model(trained_model, CONTEXT_LENGTH=5)\n",
    "        if input(\"Play again? (y/n): \").lower() != 'y':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb666641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pre-calculating optimal policy for all unique states...\n",
      "Generating all unique optimal trajectories...\n",
      "Generated 1023 unique optimal trajectories in 14.43 seconds.\n",
      "Starting training...\n",
      "Epoch 1/30, Loss: 2.1757\n",
      "Epoch 2/30, Loss: 2.0764\n",
      "Epoch 3/30, Loss: 1.9939\n",
      "Epoch 4/30, Loss: 1.8944\n",
      "Epoch 5/30, Loss: 1.7810\n",
      "Epoch 6/30, Loss: 1.6728\n",
      "Epoch 7/30, Loss: 1.5787\n",
      "Epoch 8/30, Loss: 1.5085\n",
      "Epoch 9/30, Loss: 1.4357\n",
      "Epoch 10/30, Loss: 1.3777\n",
      "Epoch 11/30, Loss: 1.3128\n",
      "Epoch 12/30, Loss: 1.2689\n",
      "Epoch 13/30, Loss: 1.2266\n",
      "Epoch 14/30, Loss: 1.1834\n",
      "Epoch 15/30, Loss: 1.1446\n",
      "Epoch 16/30, Loss: 1.1137\n",
      "Epoch 17/30, Loss: 1.0857\n",
      "Epoch 18/30, Loss: 1.0471\n",
      "Epoch 19/30, Loss: 1.0220\n",
      "Epoch 20/30, Loss: 1.0131\n",
      "Epoch 21/30, Loss: 0.9743\n",
      "Epoch 22/30, Loss: 0.9599\n",
      "Epoch 23/30, Loss: 0.9460\n",
      "Epoch 24/30, Loss: 0.9195\n",
      "Epoch 25/30, Loss: 0.9271\n",
      "Epoch 26/30, Loss: 0.8983\n",
      "Epoch 27/30, Loss: 0.8835\n",
      "Epoch 28/30, Loss: 0.8796\n",
      "Epoch 29/30, Loss: 0.8797\n",
      "Epoch 30/30, Loss: 0.8659\n",
      "Training finished.\n",
      "\n",
      "--- New Game: You are 'O' ---\n",
      " 0  1  2 \n",
      "---+---+---\n",
      " 3  4  5 \n",
      "---+---+---\n",
      " 6  7  8 \n",
      "Model's turn ('X')...\n",
      "Model chooses action: 1\n",
      "--------------------\n",
      " 0  X  2 \n",
      "---+---+---\n",
      " 3  4  5 \n",
      "---+---+---\n",
      " 6  7  8 \n",
      "--------------------\n",
      " 0  X  2 \n",
      "---+---+---\n",
      " 3  O  5 \n",
      "---+---+---\n",
      " 6  7  8 \n",
      "Model's turn ('X')...\n",
      "Model chooses action: 0\n",
      "--------------------\n",
      " X  X  2 \n",
      "---+---+---\n",
      " 3  O  5 \n",
      "---+---+---\n",
      " 6  7  8 \n",
      "--------------------\n",
      " X  X  O \n",
      "---+---+---\n",
      " 3  O  5 \n",
      "---+---+---\n",
      " 6  7  8 \n",
      "Model's turn ('X')...\n",
      "Model chooses action: 6\n",
      "--------------------\n",
      " X  X  O \n",
      "---+---+---\n",
      " 3  O  5 \n",
      "---+---+---\n",
      " X  7  8 \n",
      "--------------------\n",
      " X  X  O \n",
      "---+---+---\n",
      " O  O  5 \n",
      "---+---+---\n",
      " X  7  8 \n",
      "Model's turn ('X')...\n",
      "Model chooses action: 5\n",
      "--------------------\n",
      " X  X  O \n",
      "---+---+---\n",
      " O  O  X \n",
      "---+---+---\n",
      " X  7  8 \n",
      "Invalid move. Try again.\n",
      " X  X  O \n",
      "---+---+---\n",
      " O  O  X \n",
      "---+---+---\n",
      " X  7  8 \n",
      "--------------------\n",
      " X  X  O \n",
      "---+---+---\n",
      " O  O  X \n",
      "---+---+---\n",
      " X  7  O \n",
      "Model's turn ('X')...\n",
      "Model chooses action: 7\n",
      "--------------------\n",
      "--- Game Over ---\n",
      "It's a draw!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- 1. TicTacToe 게임 환경 (이전과 동일) ---\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        return [i for i, val in enumerate(self.get_state()) if val == 0]\n",
    "\n",
    "    def make_move(self, action):\n",
    "        if self.get_state()[action] != 0:\n",
    "            raise ValueError(\"Invalid move\")\n",
    "        row, col = action // 3, action % 3\n",
    "        self.board[row, col] = self.current_player\n",
    "        winner = self.check_winner()\n",
    "        done = winner is not None\n",
    "        reward = 0\n",
    "        if done:\n",
    "            if winner == 1: reward = 1\n",
    "            elif winner == -1: reward = -1\n",
    "            else: reward = 0\n",
    "        self.current_player *= -1\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def check_winner(self):\n",
    "        for i in range(3):\n",
    "            if abs(self.board[i, :].sum()) == 3: return self.board[i, 0]\n",
    "            if abs(self.board[:, i].sum()) == 3: return self.board[0, i]\n",
    "        if abs(np.diag(self.board).sum()) == 3: return self.board[0, 0]\n",
    "        if abs(np.diag(np.fliplr(self.board)).sum()) == 3: return self.board[0, 2]\n",
    "        if not np.any(self.board == 0): return 0\n",
    "        return None\n",
    "\n",
    "# --- 2. 최적화된 데이터 생성 ---\n",
    "\n",
    "# Minimax 알고리즘 (이전과 동일)\n",
    "def minimax(board, player):\n",
    "    game = TicTacToe()\n",
    "    game.board = board\n",
    "    winner = game.check_winner()\n",
    "    if winner is not None:\n",
    "        return winner * player\n",
    "    best_score = -math.inf\n",
    "    for action in game.get_available_actions():\n",
    "        new_board = board.copy()\n",
    "        row, col = action // 3, action % 3\n",
    "        new_board[row, col] = player\n",
    "        score = -minimax(new_board, -player)\n",
    "        if score > best_score: best_score = score\n",
    "    return best_score if best_score != -math.inf else 0\n",
    "\n",
    "def get_canonical_form(board):\n",
    "    \"\"\"보드의 8가지 대칭(회전, 대칭) 중 정규형(canonical form)을 찾습니다.\"\"\"\n",
    "    symmetries = []\n",
    "    current_board = board.copy()\n",
    "    for _ in range(4): # 4 rotations\n",
    "        symmetries.append(current_board)\n",
    "        symmetries.append(np.fliplr(current_board))\n",
    "        current_board = np.rot90(current_board)\n",
    "\n",
    "    # 튜플로 변환하여 정렬 가능하게 만듦\n",
    "    symmetries_as_tuples = [tuple(b.flatten()) for b in symmetries]\n",
    "    canonical_tuple = min(symmetries_as_tuples)\n",
    "    return np.array(canonical_tuple).reshape(3, 3)\n",
    "\n",
    "memoized_policy = {}\n",
    "def build_optimal_policy_map(board, player):\n",
    "    \"\"\"모든 고유 상태에 대한 최적의 수를 미리 계산하여 맵에 저장 (재귀)\"\"\"\n",
    "    canonical_board = get_canonical_form(board)\n",
    "    canonical_tuple = tuple(canonical_board.flatten())\n",
    "\n",
    "    if canonical_tuple in memoized_policy or TicTacToe()._check_winner_on_board(board) is not None:\n",
    "        return\n",
    "\n",
    "    game = TicTacToe()\n",
    "    game.board = board\n",
    "    available_actions = game.get_available_actions()\n",
    "    \n",
    "    best_score = -math.inf\n",
    "    best_moves = []\n",
    "    \n",
    "    for action in available_actions:\n",
    "        new_board = board.copy()\n",
    "        row, col = action // 3, action % 3\n",
    "        new_board[row, col] = player\n",
    "        score = -minimax(new_board, -player)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_moves = [action]\n",
    "        elif score == best_score:\n",
    "            best_moves.append(action)\n",
    "\n",
    "    memoized_policy[tuple(board.flatten())] = best_moves\n",
    "    \n",
    "    for move in best_moves:\n",
    "        next_board = board.copy()\n",
    "        row, col = move // 3, move % 3\n",
    "        next_board[row, col] = player\n",
    "        build_optimal_policy_map(next_board, -player)\n",
    "\n",
    "# TicTacToe 클래스 내부에 헬퍼 함수 추가\n",
    "TicTacToe._check_winner_on_board = lambda self, board: TicTacToe.check_winner(type('obj', (object,), {'board': board})())\n",
    "\n",
    "\n",
    "def generate_all_optimal_trajectories():\n",
    "    \"\"\"사전 계산된 정책을 사용하여 모든 최적의 경로를 생성합니다.\"\"\"\n",
    "    print(\"Pre-calculating optimal policy for all unique states...\")\n",
    "    start_board = np.zeros((3, 3), dtype=int)\n",
    "    build_optimal_policy_map(start_board, 1)\n",
    "    \n",
    "    print(\"Generating all unique optimal trajectories...\")\n",
    "    all_trajectories = []\n",
    "    \n",
    "    def find_paths(board, player, path):\n",
    "        winner = TicTacToe()._check_winner_on_board(board)\n",
    "        if winner is not None:\n",
    "            states, actions = zip(*path) if path else ([], [])\n",
    "            rewards = np.zeros(len(actions))\n",
    "            \n",
    "            # 승패에 따라 Reward-to-go 계산\n",
    "            rewards_to_go = np.zeros_like(rewards, dtype=float)\n",
    "            final_reward = winner # 1 for win, -1 for loss, 0 for draw\n",
    "            for t in range(len(rewards)):\n",
    "                rewards_to_go[t] = final_reward\n",
    "                \n",
    "            all_trajectories.append({\n",
    "                'states': np.array(states),\n",
    "                'actions': np.array(actions),\n",
    "                'rewards_to_go': rewards_to_go\n",
    "            })\n",
    "            return\n",
    "\n",
    "        optimal_moves = memoized_policy.get(tuple(board.flatten()))\n",
    "        if not optimal_moves: return\n",
    "\n",
    "        for move in optimal_moves:\n",
    "            next_board = board.copy()\n",
    "            row, col = move // 3, move % 3\n",
    "            next_board[row, col] = player\n",
    "            find_paths(next_board, -player, path + [(board.flatten(), move)])\n",
    "\n",
    "    find_paths(start_board, 1, [])\n",
    "    return all_trajectories\n",
    "\n",
    "\n",
    "# --- 3. PyTorch Dataset & Decision Transformer 모델 (이전과 동일) ---\n",
    "class TicTacToeDataset(Dataset):\n",
    "    def __init__(self, trajectories, context_length):\n",
    "        self.trajectories = trajectories\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = len(traj['states'])\n",
    "        start_idx = random.randint(0, traj_len - 1)\n",
    "        states = traj['states'][start_idx : start_idx + self.context_length]\n",
    "        actions = traj['actions'][start_idx : start_idx + self.context_length]\n",
    "        rtgs = traj['rewards_to_go'][start_idx : start_idx + self.context_length]\n",
    "        T = len(states)\n",
    "        padding_len = self.context_length - T\n",
    "        states = torch.tensor(np.pad(states, ((0, padding_len), (0, 0)), 'constant'), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.pad(actions, (0, padding_len), 'constant', constant_values=-1), dtype=torch.long)\n",
    "        rtgs = torch.tensor(np.pad(rtgs, (0, padding_len), 'constant'), dtype=torch.float32).unsqueeze(1)\n",
    "        mask = torch.cat([torch.ones(T), torch.zeros(padding_len)], dim=0)\n",
    "        return states, actions, rtgs, mask\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_head, n_layer, d_model, context_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.embed_state = nn.Linear(state_dim, d_model)\n",
    "        self.embed_action = nn.Embedding(action_dim + 1, d_model)\n",
    "        self.embed_rtg = nn.Linear(1, d_model)\n",
    "        self.embed_timestep = nn.Embedding(context_length, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "        self.predict_action = nn.Linear(d_model, action_dim)\n",
    "\n",
    "    def forward(self, states, actions, rtgs, timesteps):\n",
    "        action_embeddings = self.embed_action(actions + 1)\n",
    "        state_embeddings = self.embed_state(states)\n",
    "        rtg_embeddings = self.embed_rtg(rtgs)\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "        state_embeddings += time_embeddings\n",
    "        action_embeddings += time_embeddings\n",
    "        rtg_embeddings += time_embeddings\n",
    "        stacked_inputs = torch.stack(\n",
    "            (rtg_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(states.shape[0], 3 * self.context_length, self.d_model)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(3 * self.context_length).to(states.device)\n",
    "        encoder_output = self.transformer_encoder(stacked_inputs, mask=causal_mask)\n",
    "        x = encoder_output[:, 1::3, :]\n",
    "        action_preds = self.predict_action(x)\n",
    "        return action_preds\n",
    "\n",
    "# --- 4. 학습 및 평가 (하이퍼파라미터 조정) ---\n",
    "def train():\n",
    "    # Hyperparameters (조정됨)\n",
    "    CONTEXT_LENGTH = 9  # 최대 9수면 게임이 끝나므로\n",
    "    N_EPOCHS = 30       # 데이터가 고품질이므로 에포크 감소\n",
    "    BATCH_SIZE = 64     # 데이터셋 크기에 맞춰 배치 사이즈 조정\n",
    "    LR = 1e-4\n",
    "    D_MODEL = 128\n",
    "    N_HEAD = 4\n",
    "    N_LAYER = 3\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    trajectories = generate_all_optimal_trajectories()\n",
    "    end_time = time.time()\n",
    "    print(f\"Generated {len(trajectories)} unique optimal trajectories in {end_time - start_time:.2f} seconds.\")\n",
    "    \n",
    "    dataset = TicTacToeDataset(trajectories, context_length=CONTEXT_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = DecisionTransformer(\n",
    "        state_dim=9, action_dim=9, n_head=N_HEAD, n_layer=N_LAYER,\n",
    "        d_model=D_MODEL, context_length=CONTEXT_LENGTH\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for states, actions, rtgs, mask in dataloader:\n",
    "            states, actions, rtgs, mask = states.to(device), actions.to(device), rtgs.to(device), mask.to(device)\n",
    "            timesteps = torch.arange(CONTEXT_LENGTH, device=device).repeat(states.shape[0], 1)\n",
    "            action_preds = model(states, actions, rtgs, timesteps)\n",
    "            action_preds = action_preds.reshape(-1, 9)\n",
    "            actions_target = actions.reshape(-1)\n",
    "            mask = mask.reshape(-1).bool()\n",
    "            action_preds = action_preds[mask]\n",
    "            actions_target = actions_target[mask]\n",
    "            valid_targets = actions_target != -1\n",
    "            loss = loss_fn(action_preds[valid_targets], actions_target[valid_targets])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "def play_game_with_model(model, context_length):\n",
    "    model.eval()\n",
    "    \n",
    "    # 모델이 현재 사용 중인 device를 가져옵니다.\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    env = TicTacToe()\n",
    "    \n",
    "    # 모든 텐서를 생성할 때 .to(device)를 붙여줍니다.\n",
    "    states = torch.zeros((1, context_length, 9), dtype=torch.float32, device=device)\n",
    "    actions = torch.full((1, context_length), -1, dtype=torch.long, device=device)\n",
    "    rtgs = torch.zeros((1, context_length, 1), dtype=torch.float32, device=device)\n",
    "    rtgs[0, :, 0] = 1.0 # 목표는 승리(RTG=1)\n",
    "    timesteps = torch.arange(0, context_length, device=device).unsqueeze(0)\n",
    "\n",
    "    print(\"\\n--- New Game: You are 'O' ---\")\n",
    "    turn = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and turn < 9:\n",
    "        board_str = \"\"\n",
    "        for i, cell in enumerate(env.get_state()):\n",
    "            mark = 'X' if cell == 1 else 'O' if cell == -1 else str(i)\n",
    "            board_str += f\" {mark} \"\n",
    "            if (i+1) % 3 == 0:\n",
    "                board_str += \"\\n\" if i < 8 else \"\"\n",
    "                if i < 8: board_str += \"---+---+---\\n\"\n",
    "        print(board_str)\n",
    "\n",
    "        if env.current_player == 1:\n",
    "            print(\"Model's turn ('X')...\")\n",
    "            with torch.no_grad():\n",
    "                # 이제 모델과 입력 텐서가 모두 같은 device에 있습니다.\n",
    "                pred_actions = model(states, actions, rtgs, timesteps)\n",
    "            logits = pred_actions[0, turn, :]\n",
    "            available_actions = env.get_available_actions()\n",
    "            mask = torch.full_like(logits, -float('inf'))\n",
    "            mask[available_actions] = 0\n",
    "            move = (logits + mask).argmax().item()\n",
    "            print(f\"Model chooses action: {move}\")\n",
    "        else:\n",
    "            try:\n",
    "                move = int(input(\"Your turn ('O'). Enter move (0-8): \"))\n",
    "                if move not in env.get_available_actions():\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "                    continue\n",
    "            except (ValueError, IndexError):\n",
    "                print(\"Invalid input. Enter a number between 0 and 8.\")\n",
    "                continue\n",
    "\n",
    "        # 상태 업데이트 시에도 .to(device)가 필요합니다.\n",
    "        if turn < context_length:\n",
    "            current_state_tensor = torch.tensor(env.get_state(), dtype=torch.float32).to(device)\n",
    "            states[0, turn] = current_state_tensor\n",
    "            actions[0, turn] = move\n",
    "        \n",
    "        _, reward, done = env.make_move(move)\n",
    "        \n",
    "        # RTG 업데이트\n",
    "        if turn + 1 < context_length:\n",
    "            rtgs[0, turn+1:] = rtgs[0, turn:-1].clone() # clone()을 사용하여 인플레이스 수정 방지\n",
    "            rtgs[0, turn, 0] -= reward\n",
    "        \n",
    "        turn += 1\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    winner = env.check_winner()\n",
    "    print(\"--- Game Over ---\")\n",
    "    if winner == 1: print(\"Model (X) wins!\")\n",
    "    elif winner == -1: print(\"You (O) win!\")\n",
    "    else: print(\"It's a draw!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    CONTEXT_LENGTH = 9\n",
    "    trained_model = train()\n",
    "    while True:\n",
    "        play_game_with_model(trained_model, CONTEXT_LENGTH)\n",
    "        if input(\"Play again? (y/n): \").lower() != 'y':\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6182ae46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTransformer(\n",
       "  (embed_state): Linear(in_features=9, out_features=128, bias=True)\n",
       "  (embed_action): Embedding(10, 128)\n",
       "  (embed_rtg): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (embed_timestep): Embedding(9, 128)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predict_action): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4047aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Model Decision Analysis =====\n",
      "Board State (1: Model, -1: Opponent): [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Action Probabilities:\n",
      "  Move 0: 0.0799 (7.99%) [Legal]\n",
      "  Move 1: 0.6264 (62.64%) [Legal]\n",
      "  Move 2: 0.0046 (0.46%) [Legal]\n",
      "  Move 3: 0.1470 (14.70%) [Legal]\n",
      "  Move 4: 0.0363 (3.63%) [Legal]\n",
      "  Move 5: 0.0314 (3.14%) [Legal]\n",
      "  Move 6: 0.0161 (1.61%) [Legal]\n",
      "  Move 7: 0.0049 (0.49%) [Legal]\n",
      "  Move 8: 0.0533 (5.33%) [Legal]\n",
      "\n",
      "=> Model's Best Choice: Move 1 (62.64%)\n",
      "===================================\n",
      "\n",
      "===== Model Decision Analysis =====\n",
      "Board State (1: Model, -1: Opponent): [-1, -1, 0, 0, 1, 0, 0, 0, 0]\n",
      "\n",
      "Action Probabilities:\n",
      "  Move 0: 0.0204 (2.04%) [Illegal]\n",
      "  Move 1: 0.4443 (44.43%) [Illegal]\n",
      "  Move 2: 0.0428 (4.28%) [Legal]\n",
      "  Move 3: 0.1606 (16.06%) [Legal]\n",
      "  Move 4: 0.0007 (0.07%) [Illegal]\n",
      "  Move 5: 0.0475 (4.75%) [Legal]\n",
      "  Move 6: 0.0505 (5.05%) [Legal]\n",
      "  Move 7: 0.0026 (0.26%) [Legal]\n",
      "  Move 8: 0.2307 (23.07%) [Legal]\n",
      "\n",
      "=> Model's Best Choice: Move 1 (44.43%)\n",
      "===================================\n",
      "\n",
      "===== Model Decision Analysis =====\n",
      "Board State (1: Model, -1: Opponent): [1, 1, 0, -1, 0, 0, -1, 0, 0]\n",
      "\n",
      "Action Probabilities:\n",
      "  Move 0: 0.0175 (1.75%) [Illegal]\n",
      "  Move 1: 0.1842 (18.42%) [Illegal]\n",
      "  Move 2: 0.0290 (2.90%) [Legal]\n",
      "  Move 3: 0.0449 (4.49%) [Illegal]\n",
      "  Move 4: 0.0891 (8.91%) [Legal]\n",
      "  Move 5: 0.1057 (10.57%) [Legal]\n",
      "  Move 6: 0.1907 (19.07%) [Illegal]\n",
      "  Move 7: 0.2156 (21.56%) [Legal]\n",
      "  Move 8: 0.1234 (12.34%) [Legal]\n",
      "\n",
      "=> Model's Best Choice: Move 7 (21.56%)\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0175, 0.1842, 0.0290, 0.0449, 0.0891, 0.1057, 0.1907, 0.2156, 0.1234],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_model_decision(model, board_state, context_length=9):\n",
    "    \"\"\"\n",
    "    주어진 보드 상태에서 모델이 각 행동에 대해 어떻게 평가하는지 분석합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 모델 입력을 위한 빈 시퀀스 준비\n",
    "    states = torch.zeros((1, context_length, 9), dtype=torch.float32, device=device)\n",
    "    actions = torch.full((1, context_length), -1, dtype=torch.long, device=device)\n",
    "    rtgs = torch.zeros((1, context_length, 1), dtype=torch.float32, device=device)\n",
    "    rtgs[0, :, 0] = 1.0  # 항상 '승리'를 목표로 결정 분석\n",
    "    timesteps = torch.arange(0, context_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # 현재 상태를 시퀀스의 첫 번째(turn=0)에 배치\n",
    "    states[0, 0] = torch.tensor(board_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_actions_logits = model(states, actions, rtgs, timesteps)\n",
    "        \n",
    "    # 첫 번째 턴에 대한 결정(logits)을 가져옴\n",
    "    logits = pred_actions_logits[0, 0, :]\n",
    "    \n",
    "    # Softmax를 이용해 확률로 변환\n",
    "    probabilities = F.softmax(logits, dim=0)\n",
    "    \n",
    "    print(\"===== Model Decision Analysis =====\")\n",
    "    print(\"Board State (1: Model, -1: Opponent):\", board_state)\n",
    "    print(\"\\nAction Probabilities:\")\n",
    "    \n",
    "    # 가능한 수와 불가능한 수 구분하여 출력\n",
    "    available_actions = [i for i, val in enumerate(board_state) if val == 0]\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        move_type = \"Legal\" if i in available_actions else \"Illegal\"\n",
    "        print(f\"  Move {i}: {prob.item():.4f} ({prob.item()*100:.2f}%) [{move_type}]\")\n",
    "        \n",
    "    best_move = probabilities.argmax().item()\n",
    "    print(f\"\\n=> Model's Best Choice: Move {best_move} ({probabilities[best_move].item()*100:.2f}%)\")\n",
    "    print(\"===================================\\n\")\n",
    "    return probabilities\n",
    "\n",
    "# trained_model이 학습 완료된 모델이라고 가정\n",
    "# 예제 분석 실행\n",
    "# 1. 빈 보드 (첫 수 분석)\n",
    "empty_board = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "analyze_model_decision(trained_model, empty_board)\n",
    "\n",
    "# 2. 상대방(O)이 2개를 놓아 반드시 막아야 하는 상황\n",
    "must_block_board = [ -1, -1, 0,  # O, O, _\n",
    "                      0, 1, 0, \n",
    "                      0, 0, 0]\n",
    "analyze_model_decision(trained_model, must_block_board)\n",
    "\n",
    "# 3. 내가(X) 2개를 놓아 바로 이길 수 있는 상황\n",
    "can_win_board = [ 1, 1, 0,  # X, X, _\n",
    "                 -1, 0, 0, \n",
    "                 -1, 0, 0]\n",
    "analyze_model_decision(trained_model, can_win_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a9580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
